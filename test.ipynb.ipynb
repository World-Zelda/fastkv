{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704395db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import comb\n",
    "\n",
    "def pass_at_k(results: np.ndarray, k: int) -> float:\n",
    "    n, m = results.shape\n",
    "    if k > n:\n",
    "        return np.nan\n",
    "    sample_results = results.T\n",
    "    total_pass = 0.0\n",
    "    for i in range(m):\n",
    "        successes = int(np.sum(sample_results[i]))\n",
    "        failures = n - successes\n",
    "        if successes >= k:\n",
    "            total_pass += 1.0\n",
    "        elif failures >= k:\n",
    "            fail_comb = comb(failures, k)\n",
    "            total_comb = comb(n, k)\n",
    "            total_pass += 1.0 - (fail_comb / total_comb)\n",
    "        else:\n",
    "            total_pass += 1.0\n",
    "    return total_pass / m\n",
    "\n",
    "def load_csvs_from_dir(dir_path: str) -> np.ndarray:\n",
    "    csv_files = [f for f in os.listdir(dir_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        raise ValueError(\"No CSV files\")\n",
    "    \n",
    "    all_data = []\n",
    "    for fname in sorted(csv_files):\n",
    "        df = pd.read_csv(os.path.join(dir_path, fname))\n",
    "        if 'global_index' not in df.columns or 'score' not in df.columns:\n",
    "            raise ValueError(f\"Missing columns in {fname}\")\n",
    "        df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0).astype(int)\n",
    "        df = df[['global_index', 'score']].set_index('global_index')\n",
    "        all_data.append(df)\n",
    "    \n",
    "    all_indices = sorted(set(idx for df in all_data for idx in df.index))\n",
    "    aligned = [df.reindex(all_indices, fill_value=0)['score'].values for df in all_data]\n",
    "    return np.array(aligned)\n",
    "\n",
    "def extract_metadata_robust(leaf_dir: str):\n",
    "    \"\"\"\n",
    "    ä»å¶å­ç›®å½•è·¯å¾„ä¸­æå– model, dataset, temperature\n",
    "    ç‰¹æ®Šå¤„ç† AIME2025-I / AIME2025-II\n",
    "    \"\"\"\n",
    "    parts = os.path.normpath(leaf_dir).split(os.sep)\n",
    "    \n",
    "    # åˆå§‹åŒ–\n",
    "    model = dataset = temp_str = \"unknown\"\n",
    "\n",
    "    # æŸ¥æ‰¾ AIME2025 ç‰¹æ®Šç»“æ„\n",
    "    aime_handled = False\n",
    "    for i in range(len(parts) - 1):\n",
    "        if parts[i] == \"AIME2025\":\n",
    "            if i + 1 < len(parts):\n",
    "                next_part = parts[i + 1]\n",
    "                if next_part in (\"AIME2025-I\", \"AIME2025-II\"):\n",
    "                    # æ‰¾åˆ°å­æ•°æ®é›†\n",
    "                    dataset = next_part\n",
    "                    # model æ˜¯ AIME2025 çš„ä¸Šä¸€çº§\n",
    "                    if i - 1 >= 0:\n",
    "                        model = parts[i - 1]\n",
    "                    # temperature åœ¨å­æ•°æ®é›†ä¹‹å\n",
    "                    if i + 2 < len(parts):\n",
    "                        temp_candidate = parts[i + 2]\n",
    "                        # åˆ¤æ–­æ˜¯å¦ä¸ºæ¸©åº¦ï¼ˆå¯èƒ½æ˜¯æ•°å­—æˆ– rangeï¼‰\n",
    "                        # å¦‚æœä¸‹ä¸‹çº§åƒ rangeï¼ˆå¦‚ 0-100ï¼‰ï¼Œåˆ™å†ä¸‹ä¸€çº§æ‰æ˜¯æ¸©åº¦ï¼Ÿ\n",
    "                        # ä½†é€šå¸¸ç»“æ„æ˜¯: .../model/AIME2025/AIME2025-I/1.0/[0-100]/\n",
    "                        # æ‰€ä»¥æ¸©åº¦åœ¨ i+2\n",
    "                        temp_str = temp_candidate\n",
    "                    else:\n",
    "                        temp_str = \"unknown\"\n",
    "                    aime_handled = True\n",
    "                    break\n",
    "\n",
    "    if not aime_handled:\n",
    "        # æ™®é€šé€»è¾‘ï¼šå°è¯•ä»æœ«å°¾æå–\n",
    "        if len(parts) >= 4:\n",
    "            last_part = parts[-1]\n",
    "            # åˆ¤æ–­æœ€åä¸€çº§æ˜¯å¦ä¸º rangeï¼ˆå«æ•°å­—å’Œ -ï¼‰\n",
    "            if '-' in last_part and any(c.isdigit() for c in last_part):\n",
    "                # å¯èƒ½æ˜¯ range ç›®å½•ï¼Œè·³è¿‡\n",
    "                if len(parts) >= 5:\n",
    "                    model = parts[-4]\n",
    "                    dataset = parts[-3]\n",
    "                    temp_str = parts[-2]\n",
    "                else:\n",
    "                    model = parts[-3]\n",
    "                    dataset = parts[-2]\n",
    "                    temp_str = parts[-1]\n",
    "            else:\n",
    "                # æœ€åä¸€çº§ä¸æ˜¯ range\n",
    "                model = parts[-3]\n",
    "                dataset = parts[-2]\n",
    "                temp_str = parts[-1]\n",
    "        elif len(parts) >= 3:\n",
    "            model = parts[-3]\n",
    "            dataset = parts[-2]\n",
    "            temp_str = parts[-1]\n",
    "        else:\n",
    "            model = dataset = temp_str = \"unknown\"\n",
    "\n",
    "    # è§£æ temperature\n",
    "    try:\n",
    "        temperature = float(temp_str)\n",
    "    except (ValueError, TypeError):\n",
    "        temperature = temp_str\n",
    "\n",
    "    return model, dataset, temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efc67bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†: results/Qwen3-0.6B/MATH-500\n",
      "âš ï¸ è·³è¿‡ results/Qwen3-0.6B/MATH-500: Missing columns in run1.csv\n",
      "å¤„ç†: results/Qwen3-0.6B/MATH-500/0.4\n",
      "å¤„ç†: results/Qwen3-0.6B/MATH-500/0.4/0-500\n",
      "å¤„ç†: results/Qwen3-0.6B/MATH-500/0.6/0-500\n",
      "å¤„ç†: results/Qwen3-0.6B/MATH-500/0.8\n",
      "âš ï¸ è·³è¿‡ results/Qwen3-0.6B/MATH-500/0.8: Missing columns in run1.csv\n",
      "å¤„ç†: results/Qwen3-0.6B/MATH-500/1.0/0-500\n",
      "å¤„ç†: results/Qwen3-0.6B/MATH-500/1.2\n",
      "\n",
      "âœ… æˆåŠŸæ±‡æ€» 5 ä¸ªå®éªŒç»“æœåˆ° qwen500.csv\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"results/Qwen3-0.6B/MATH-500\"\n",
    "output_csv = \"qwen500.csv\"\n",
    "rows = []\n",
    "\n",
    "# éå†æ‰€æœ‰ç›®å½•ï¼Œæ‰¾å‡ºåŒ…å« .csv çš„å¶å­ç›®å½•\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    csv_files = [f for f in filenames if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        continue\n",
    "\n",
    "    print(f\"å¤„ç†: {dirpath}\")\n",
    "    try:\n",
    "        results = load_csvs_from_dir(dirpath)\n",
    "        n_runs, n_samples = results.shape\n",
    "        overall_acc = float(np.mean(results))\n",
    "\n",
    "        model, dataset, temperature = extract_metadata_robust(dirpath)\n",
    "\n",
    "        # è®¡ç®— pass@k\n",
    "        pass1 = pass_at_k(results, 1)\n",
    "        pass3 = pass_at_k(results, 3) if n_runs >= 3 else np.nan\n",
    "        pass5 = pass_at_k(results, 5) if n_runs >= 5 else np.nan\n",
    "        pass10 = pass_at_k(results, 10) if n_runs >= 10 else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": model,\n",
    "            \"dataset\": dataset,\n",
    "            \"temperature\": temperature,\n",
    "            \"num_runs\": n_runs,\n",
    "            \"num_samples\": n_samples,\n",
    "            \"overall_accuracy\": round(overall_acc, 6),\n",
    "            \"pass@1\": round(pass1, 6),\n",
    "            \"pass@3\": round(pass3, 6) if not np.isnan(pass3) else None,\n",
    "            \"pass@5\": round(pass5, 6) if not np.isnan(pass5) else None,\n",
    "            \"pass@10\": round(pass10, 6) if not np.isnan(pass10) else None,\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è·³è¿‡ {dirpath}: {e}\")\n",
    "        continue\n",
    "\n",
    "# ä¿å­˜\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "    print(f\"\\nâœ… æˆåŠŸæ±‡æ€» {len(df)} ä¸ªå®éªŒç»“æœåˆ° {output_csv}\")\n",
    "else:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå®éªŒç›®å½•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a18d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Scanning temperature 0.4 at results/Qwen3-0.6B/MATH-500/0.4\n",
      "   Found 10 CSV files.\n",
      "ğŸ” Scanning temperature 0.6 at results/Qwen3-0.6B/MATH-500/0.6\n",
      "   Found 10 CSV files.\n",
      "ğŸ” Scanning temperature 0.8 at results/Qwen3-0.6B/MATH-500/0.8\n",
      "   Found 10 CSV files.\n",
      "ğŸ” Scanning temperature 1.0 at results/Qwen3-0.6B/MATH-500/1.0\n",
      "   Found 10 CSV files.\n",
      "ğŸ” Scanning temperature 1.2 at results/Qwen3-0.6B/MATH-500/1.2\n",
      "   Found 10 CSV files.\n",
      "\n",
      "âœ… Total runs loaded: 50\n",
      "ğŸ“Š Unique samples: 500 (min=0, max=499)\n",
      "ğŸ§® Result matrix shape: (25000, 500)\n",
      "\n",
      "âœ… Successfully saved results to: MATH-500_passk.csv\n",
      "\n",
      "Result:\n",
      " dataset  num_runs  num_samples  overall_accuracy  pass@1  pass@3   pass@5  pass@10  pass@50\n",
      "MATH-500     25000          500          0.000903   0.922 0.86601 0.830042  0.75829 0.043858\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import comb\n",
    "\n",
    "# ----------------------------\n",
    "# é…ç½®è·¯å¾„å’Œå‚æ•°\n",
    "# ----------------------------\n",
    "BASE_DIR = \"results/Qwen3-0.6B/MATH-500\"\n",
    "TEMPERATURES = [0.4, 0.6, 0.8, 1.0, 1.2]\n",
    "OUTPUT_CSV = \"MATH-500_passk.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# å·¥å…·å‡½æ•°ï¼šè®¡ç®— pass@k\n",
    "# ----------------------------\n",
    "\n",
    "def pass_at_k(results: np.ndarray, k: int) -> float:\n",
    "    n, m = results.shape\n",
    "    if k > n:\n",
    "        return float('nan')\n",
    "    total_pass = 0.0\n",
    "    for i in range(m):\n",
    "        successes = int(np.sum(results[:, i]))\n",
    "        failures = n - successes\n",
    "        if successes >= k:\n",
    "            total_pass += 1.0\n",
    "        elif failures >= k:\n",
    "            # æ‰€æœ‰ k ä¸ªéƒ½å¤±è´¥çš„æ¦‚ç‡\n",
    "            prob_all_fail = comb(failures, k) / comb(n, k)\n",
    "            total_pass += 1.0 - prob_all_fail\n",
    "        else:\n",
    "            total_pass += 1.0  # ä¸å¯èƒ½å…¨å¤±è´¥\n",
    "    return total_pass / m\n",
    "\n",
    "# ----------------------------\n",
    "# ä¸»ç¨‹åº\n",
    "# ----------------------------\n",
    "all_runs_data = []  # å­˜æ”¾æ¯ä¸ª run çš„ (global_index, score)\n",
    "\n",
    "for temp in TEMPERATURES:\n",
    "    temp_str = str(temp)\n",
    "    temp_path = os.path.join(BASE_DIR, temp_str)\n",
    "    \n",
    "    if not os.path.exists(temp_path):\n",
    "        raise FileNotFoundError(f\"âŒ Temperature directory not found: {temp_path}\")\n",
    "    \n",
    "    print(f\"ğŸ” Scanning temperature {temp} at {temp_path}\")\n",
    "    \n",
    "    # é€’å½’æŸ¥æ‰¾è¯¥æ¸©åº¦ä¸‹æ‰€æœ‰ .csv æ–‡ä»¶\n",
    "    csv_files = []\n",
    "    for root, _, files in os.walk(temp_path):\n",
    "        for f in files:\n",
    "            if f.endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, f))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"âŒ No CSV files found under {temp_path}\")\n",
    "    \n",
    "    print(f\"   Found {len(csv_files)} CSV files.\")\n",
    "    \n",
    "    # è¯»å–æ¯ä¸ª CSV\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if 'global_index' not in df.columns or 'score' not in df.columns:\n",
    "                raise ValueError(f\"File {csv_file} missing 'global_index' or 'score'\")\n",
    "            # è½¬æ¢ score ä¸ºæ•´æ•°ï¼ˆ0/1ï¼‰\n",
    "            df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0).astype(int)\n",
    "            # åªä¿ç•™éœ€è¦çš„åˆ—\n",
    "            all_runs_data.append(df[['global_index', 'score']].copy())\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipping file {csv_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nâœ… Total runs loaded: {len(all_runs_data)}\")\n",
    "\n",
    "if len(all_runs_data) == 0:\n",
    "    raise RuntimeError(\"No valid runs loaded!\")\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰ runs\n",
    "full_df = pd.concat(all_runs_data, ignore_index=True)\n",
    "\n",
    "# è·å–æ‰€æœ‰å”¯ä¸€ global_index å¹¶æ’åºï¼ˆåº”ä¸º 0~499ï¼‰\n",
    "unique_indices = sorted(full_df['global_index'].unique())\n",
    "print(f\"ğŸ“Š Unique samples: {len(unique_indices)} (min={min(unique_indices)}, max={max(unique_indices)})\")\n",
    "\n",
    "# æ„å»ºæ˜ å°„ï¼šglobal_index â†’ column index\n",
    "index_to_col = {idx: i for i, idx in enumerate(unique_indices)}\n",
    "num_samples = len(unique_indices)\n",
    "num_runs = len(full_df)\n",
    "\n",
    "# åˆå§‹åŒ–ç»“æœçŸ©é˜µ (num_runs Ã— num_samples)\n",
    "matrix = np.zeros((num_runs, num_samples), dtype=int)\n",
    "\n",
    "# å¡«å……çŸ©é˜µ\n",
    "for i, (_, row) in enumerate(full_df.iterrows()):\n",
    "    col = index_to_col[row['global_index']]\n",
    "    matrix[i, col] = row['score']\n",
    "\n",
    "print(f\"ğŸ§® Result matrix shape: {matrix.shape}\")\n",
    "\n",
    "# è®¡ç®—æŒ‡æ ‡\n",
    "overall_acc = float(np.mean(matrix))\n",
    "pass1 = pass_at_k(matrix, 1)\n",
    "pass3 = pass_at_k(matrix, 3)\n",
    "pass5 = pass_at_k(matrix, 5)\n",
    "pass10 = pass_at_k(matrix, 10)\n",
    "pass50 = pass_at_k(matrix, 50)\n",
    "\n",
    "# å‡†å¤‡è¾“å‡º\n",
    "result_row = {\n",
    "    \"dataset\": \"MATH-500\",\n",
    "    \"num_runs\": num_runs,\n",
    "    \"num_samples\": num_samples,\n",
    "    \"overall_accuracy\": round(overall_acc, 6),\n",
    "    \"pass@1\": round(pass1, 6),\n",
    "    \"pass@3\": round(pass3, 6),\n",
    "    \"pass@5\": round(pass5, 6),\n",
    "    \"pass@10\": round(pass10, 6),\n",
    "    \"pass@50\": round(pass50, 6),\n",
    "}\n",
    "\n",
    "# ä¿å­˜åˆ° CSV\n",
    "output_df = pd.DataFrame([result_row])\n",
    "output_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "print(f\"\\nâœ… Successfully saved results to: {OUTPUT_CSV}\")\n",
    "print(\"\\nResult:\")\n",
    "print(output_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80306a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Found 5 datasets to process:\n",
      "  - AIME2025-I\n",
      "  - AIME2025-II\n",
      "  - aime_2024\n",
      "  - HMMT_2025\n",
      "  - MATH-500\n",
      "\n",
      "ğŸš€ Processing dataset: AIME2025-I\n",
      "  â†’ Matrix shape: (630, 15)\n",
      "\n",
      "ğŸš€ Processing dataset: AIME2025-II\n",
      "  â†’ Matrix shape: (630, 15)\n",
      "\n",
      "ğŸš€ Processing dataset: aime_2024\n",
      "  â†’ Matrix shape: (1500, 30)\n",
      "\n",
      "ğŸš€ Processing dataset: HMMT_2025\n",
      "  â†’ Matrix shape: (1230, 30)\n",
      "\n",
      "ğŸš€ Processing dataset: MATH-500\n",
      "  â†’ Matrix shape: (25000, 500)\n",
      "\n",
      "âœ… Successfully saved 5 results to qwen0.6b_all_datasets_passk.csv\n",
      "\n",
      "Summary:\n",
      "    dataset  num_runs  num_samples  overall_accuracy   pass@1   pass@3   pass@5  pass@10  pass@50\n",
      " AIME2025-I       630           15          0.007831 0.533333 0.466984 0.271927 0.277113 0.239890\n",
      "AIME2025-II       630           15          0.004656 0.200000 0.200000 0.200000 0.138475 0.129466\n",
      "  aime_2024      1500           30          0.000844 0.333333 0.200333 0.134664 0.008329 0.039224\n",
      "  HMMT_2025      1230           30          0.001572 0.266667 0.100407 0.100678 0.069357 0.055224\n",
      "   MATH-500     25000          500          0.000903 0.922000 0.866010 0.830042 0.758290 0.043858\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import comb\n",
    "\n",
    "# ----------------------------\n",
    "# é…ç½®\n",
    "# ----------------------------\n",
    "MODEL_ROOT = \"results/Qwen3-0.6B\"\n",
    "TEMPERATURES = [0.4, 0.6, 0.8, 1.0, 1.2]\n",
    "OUTPUT_CSV = \"qwen0.6b_all_datasets_passk.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# å·¥å…·å‡½æ•°ï¼šè®¡ç®— pass@k\n",
    "# ----------------------------\n",
    "\n",
    "def pass_at_k(results: np.ndarray, k: int) -> float:\n",
    "    n, m = results.shape\n",
    "    if k > n:\n",
    "        return float('nan')\n",
    "    total_pass = 0.0\n",
    "    for i in range(m):\n",
    "        successes = int(np.sum(results[:, i]))\n",
    "        failures = n - successes\n",
    "        if successes >= k:\n",
    "            total_pass += 1.0\n",
    "        elif failures >= k:\n",
    "            prob_all_fail = comb(failures, k) / comb(n, k)\n",
    "            total_pass += 1.0 - prob_all_fail\n",
    "        else:\n",
    "            total_pass += 1.0\n",
    "    return total_pass / m\n",
    "\n",
    "# ----------------------------\n",
    "# åŠ è½½å•ä¸ªæ•°æ®é›†ï¼ˆæˆ–å­æ•°æ®é›†ï¼‰çš„æ‰€æœ‰ runs\n",
    "# ----------------------------\n",
    "\n",
    "def load_dataset_runs(dataset_path: str):\n",
    "    \"\"\"ä» dataset_pathï¼ˆå¦‚ .../MATH-500 æˆ– .../AIME2025/AIME2025-Iï¼‰åŠ è½½æ‰€æœ‰æ¸©åº¦ä¸‹çš„ CSV\"\"\"\n",
    "    all_dfs = []\n",
    "    for temp in TEMPERATURES:\n",
    "        temp_str = str(temp)\n",
    "        temp_dir = os.path.join(dataset_path, temp_str)\n",
    "        if not os.path.exists(temp_dir):\n",
    "            print(f\"  âš ï¸ Temperature {temp} missing for {os.path.basename(dataset_path)}\")\n",
    "            continue\n",
    "\n",
    "        # é€’å½’æ‰¾æ‰€æœ‰ .csv\n",
    "        csv_files = []\n",
    "        for root, _, files in os.walk(temp_dir):\n",
    "            for f in files:\n",
    "                if f.endswith('.csv'):\n",
    "                    csv_files.append(os.path.join(root, f))\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"  âš ï¸ No CSVs found for temp {temp}\")\n",
    "            continue\n",
    "\n",
    "        # è¯»å–æ¯ä¸ª CSV\n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                if 'global_index' not in df.columns or 'score' not in df.columns:\n",
    "                    raise ValueError(f\"Missing columns in {csv_file}\")\n",
    "                df['score'] = pd.to_numeric(df['score'], errors='coerce').fillna(0).astype(int)\n",
    "                all_dfs.append(df[['global_index', 'score']].copy())\n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ Skip {csv_file}: {e}\")\n",
    "    \n",
    "    if not all_dfs:\n",
    "        return None\n",
    "    \n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# ä¸»ç¨‹åº\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "if not os.path.exists(MODEL_ROOT):\n",
    "    raise FileNotFoundError(f\"Model root not found: {MODEL_ROOT}\")\n",
    "\n",
    "# æ„å»ºè¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨\n",
    "datasets_to_process = []\n",
    "\n",
    "# éå† Qwen3-0.6B ä¸‹çš„ä¸€çº§ç›®å½•\n",
    "for item in os.listdir(MODEL_ROOT):\n",
    "    item_path = os.path.join(MODEL_ROOT, item)\n",
    "    if not os.path.isdir(item_path):\n",
    "        continue\n",
    "\n",
    "    if item == \"AIME2025\":\n",
    "        # ç‰¹æ®Šå¤„ç†ï¼šæ‹†æˆä¸¤ä¸ªå­é›†\n",
    "        for sub in [\"AIME2025-I\", \"AIME2025-II\"]:\n",
    "            sub_path = os.path.join(item_path, sub)\n",
    "            if os.path.isdir(sub_path):\n",
    "                datasets_to_process.append((sub, sub_path))\n",
    "            else:\n",
    "                print(f\"âš ï¸ Sub-dataset {sub} not found under AIME2025\")\n",
    "    else:\n",
    "        # æ™®é€šæ•°æ®é›†\n",
    "        datasets_to_process.append((item, item_path))\n",
    "\n",
    "print(f\"ğŸ“ Found {len(datasets_to_process)} datasets to process:\")\n",
    "for name, _ in datasets_to_process:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset_name, dataset_path in datasets_to_process:\n",
    "    print(f\"\\nğŸš€ Processing dataset: {dataset_name}\")\n",
    "    full_df = load_dataset_runs(dataset_path)\n",
    "    \n",
    "    if full_df is None or full_df.empty:\n",
    "        print(f\"  âŒ No valid data for {dataset_name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # è·å–å”¯ä¸€ global_index å¹¶æ’åº\n",
    "    unique_indices = sorted(full_df['global_index'].unique())\n",
    "    index_to_col = {idx: i for i, idx in enumerate(unique_indices)}\n",
    "    num_samples = len(unique_indices)\n",
    "    num_runs = len(full_df)\n",
    "\n",
    "    # æ„å»ºçŸ©é˜µ\n",
    "    matrix = np.zeros((num_runs, num_samples), dtype=int)\n",
    "    for i, (_, row) in enumerate(full_df.iterrows()):\n",
    "        col = index_to_col[row['global_index']]\n",
    "        matrix[i, col] = row['score']\n",
    "\n",
    "    print(f\"  â†’ Matrix shape: ({num_runs}, {num_samples})\")\n",
    "\n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    overall_acc = float(np.mean(matrix))\n",
    "    pass1 = pass_at_k(matrix, 1)\n",
    "    pass3 = pass_at_k(matrix, 3)\n",
    "    pass5 = pass_at_k(matrix, 5)\n",
    "    pass10 = pass_at_k(matrix, 10)\n",
    "    pass50 = pass_at_k(matrix, 50)\n",
    "\n",
    "    results.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"num_runs\": num_runs,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"overall_accuracy\": round(overall_acc, 6),\n",
    "        \"pass@1\": round(pass1, 6),\n",
    "        \"pass@3\": round(pass3, 6),\n",
    "        \"pass@5\": round(pass5, 6),\n",
    "        \"pass@10\": round(pass10, 6),\n",
    "        \"pass@50\": round(pass50, 6),\n",
    "    })\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "if results:\n",
    "    df_out = pd.DataFrame(results)\n",
    "    df_out.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "    print(f\"\\nâœ… Successfully saved {len(results)} results to {OUTPUT_CSV}\")\n",
    "    print(\"\\nSummary:\")\n",
    "    print(df_out.to_string(index=False))\n",
    "else:\n",
    "    print(\"âŒ No results generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
