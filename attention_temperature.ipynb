{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959e7063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xsj/data_xsj/miniconda3/envs/fast/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af56869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用您提供的健壮答案处理代码（保持不变）\n",
    "def last_boxed_only_string(string):\n",
    "    \"\"\"提取最后一个 \\\\boxed 内容\"\"\"\n",
    "    if string is None:\n",
    "        return None\n",
    "    idx = string.rfind(\"\\\\boxed\")\n",
    "    if \"\\\\boxed \" in string:\n",
    "        parts = string.split(\"\\\\boxed \")\n",
    "        if len(parts) > 1:\n",
    "            return \"\\\\boxed \" + parts[-1].split(\"$\")[0].split(\"\\n\")[0]\n",
    "    if idx < 0:\n",
    "        idx = string.rfind(\"\\\\fbox\")\n",
    "        if idx < 0:\n",
    "            return None\n",
    "\n",
    "    i = idx\n",
    "    right_brace_idx = None\n",
    "    num_left_braces_open = 0\n",
    "    while i < len(string):\n",
    "        if string[i] == \"{\":\n",
    "            num_left_braces_open += 1\n",
    "        if string[i] == \"}\":\n",
    "            num_left_braces_open -= 1\n",
    "            if num_left_braces_open == 0:\n",
    "                right_brace_idx = i\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    retval = None if right_brace_idx is None else string[idx : right_brace_idx + 1]\n",
    "    return retval\n",
    "\n",
    "def remove_boxed(s):\n",
    "    \"\"\"移除 \\\\boxed 包装\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    if \"\\\\boxed \" in s:\n",
    "        left = \"\\\\boxed \"\n",
    "        if s.startswith(left):\n",
    "            return s[len(left):]\n",
    "        return s\n",
    "\n",
    "    left = \"\\\\boxed{\"\n",
    "    if s.startswith(left) and s.endswith(\"}\"):\n",
    "        return s[len(left):-1]\n",
    "    return s\n",
    "\n",
    "def fix_fracs(string):\n",
    "    \"\"\"修复分数格式\"\"\"\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    substrs = string.split(\"\\\\frac\")\n",
    "    new_str = substrs[0]\n",
    "    if len(substrs) > 1:\n",
    "        substrs = substrs[1:]\n",
    "        for substr in substrs:\n",
    "            new_str += \"\\\\frac\"\n",
    "            if substr[0] == \"{\":\n",
    "                new_str += substr\n",
    "            else:\n",
    "                try:\n",
    "                    assert len(substr) >= 2\n",
    "                except:\n",
    "                    return string\n",
    "                a = substr[0]\n",
    "                b = substr[1]\n",
    "                if b != \"{\":\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\" + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}{\" + b + \"}\"\n",
    "                else:\n",
    "                    if len(substr) > 2:\n",
    "                        post_substr = substr[2:]\n",
    "                        new_str += \"{\" + a + \"}\" + b + post_substr\n",
    "                    else:\n",
    "                        new_str += \"{\" + a + \"}\" + b\n",
    "    string = new_str\n",
    "    return string\n",
    "\n",
    "def fix_a_slash_b(string):\n",
    "    \"\"\"修复 a/b 格式为分数\"\"\"\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    if len(string.split(\"/\")) != 2:\n",
    "        return string\n",
    "    a = string.split(\"/\")[0]\n",
    "    b = string.split(\"/\")[1]\n",
    "    try:\n",
    "        a = int(a)\n",
    "        b = int(b)\n",
    "        assert string == \"{}/{}\".format(a, b)\n",
    "        new_string = \"\\\\frac{\" + str(a) + \"}{\" + str(b) + \"}\"\n",
    "        return new_string\n",
    "    except:\n",
    "        return string\n",
    "\n",
    "def remove_right_units(string):\n",
    "    \"\"\"移除右侧单位\"\"\"\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    if \"\\\\text{ \" in string:\n",
    "        splits = string.split(\"\\\\text{ \")\n",
    "        assert len(splits) == 2\n",
    "        return splits[0]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def fix_sqrt(string):\n",
    "    \"\"\"修复平方根格式\"\"\"\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "    if \"\\\\sqrt\" not in string:\n",
    "        return string\n",
    "    splits = string.split(\"\\\\sqrt\")\n",
    "    new_string = splits[0]\n",
    "    for split in splits[1:]:\n",
    "        if split[0] != \"{\":\n",
    "            a = split[0]\n",
    "            new_substr = \"\\\\sqrt{\" + a + \"}\" + split[1:]\n",
    "        else:\n",
    "            new_substr = \"\\\\sqrt\" + split\n",
    "        new_string += new_substr\n",
    "    return new_string\n",
    "\n",
    "def strip_string(string):\n",
    "    \"\"\"标准化字符串\"\"\"\n",
    "    if string is None:\n",
    "        return \"\"\n",
    "        \n",
    "    # linebreaks\n",
    "    string = string.replace(\"\\n\", \"\")\n",
    "\n",
    "    # remove inverse spaces\n",
    "    string = string.replace(\"\\\\!\", \"\")\n",
    "\n",
    "    # replace \\\\ with \\\n",
    "    string = string.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "\n",
    "    # replace tfrac and dfrac with frac\n",
    "    string = string.replace(\"tfrac\", \"frac\")\n",
    "    string = string.replace(\"dfrac\", \"frac\")\n",
    "\n",
    "    # remove \\left and \\right\n",
    "    string = string.replace(\"\\\\left\", \"\")\n",
    "    string = string.replace(\"\\\\right\", \"\")\n",
    "\n",
    "    # Remove circ (degrees)\n",
    "    string = string.replace(\"^{\\\\circ}\", \"\")\n",
    "    string = string.replace(\"^\\\\circ\", \"\")\n",
    "\n",
    "    # remove dollar signs\n",
    "    string = string.replace(\"\\\\$\", \"\")\n",
    "\n",
    "    # 新增：去除\\text{...}，保留内容\n",
    "    import re\n",
    "    string = re.sub(r'\\\\text{([^}]*)}', r'\\1', string)\n",
    "\n",
    "    # remove units (on the right)\n",
    "    string = remove_right_units(string)\n",
    "\n",
    "    # remove percentage\n",
    "    string = string.replace(\"\\\\%\", \"\")\n",
    "    string = string.replace(\"\\%\", \"\")\n",
    "\n",
    "    # \" 0.\" equivalent to \" .\" and \"{0.\" equivalent to \"{.\" Alternatively, add \"0\" if \".\" is the start of the string\n",
    "    string = string.replace(\" .\", \" 0.\")\n",
    "    string = string.replace(\"{.\", \"{0.\")\n",
    "    # if empty, return empty string\n",
    "    if len(string) == 0:\n",
    "        return string\n",
    "    if string[0] == \".\":\n",
    "        string = \"0\" + string\n",
    "\n",
    "    # to consider: get rid of e.g. \"k = \" or \"q = \" at beginning\n",
    "    if len(string.split(\"=\")) == 2 and len(string.split(\"=\")[0]) <= 2:\n",
    "        string = string.split(\"=\")[1]\n",
    "\n",
    "    # fix sqrt3 --> sqrt{3}\n",
    "    string = fix_sqrt(string)\n",
    "\n",
    "    # remove spaces\n",
    "    string = string.replace(\" \", \"\")\n",
    "\n",
    "    # \\frac1b or \\frac12 --> \\frac{1}{b} and \\frac{1}{2}, etc. Even works with \\frac1{72} (but not \\frac{72}1).\n",
    "    # Also does a/b --> \\\\frac{a}{b}\n",
    "    string = fix_fracs(string)\n",
    "\n",
    "    # manually change 0.5 --> \\frac{1}{2}\n",
    "    if string == \"0.5\":\n",
    "        string = \"\\\\frac{1}{2}\"\n",
    "\n",
    "    # NOTE: X/Y changed to \\frac{X}{Y} in dataset, but in simple cases fix in case the model输出 is X/Y\n",
    "    string = fix_a_slash_b(string)\n",
    "\n",
    "    return string\n",
    "\n",
    "def is_equiv(str1, str2, verbose=False):\n",
    "    \"\"\"比较两个字符串是否等价\"\"\"\n",
    "    if str1 is None and str2 is None:\n",
    "        if verbose:\n",
    "            print(\"WARNING: Both None\")\n",
    "        return True\n",
    "    if str1 is None or str2 is None:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        ss1 = strip_string(str1)\n",
    "        ss2 = strip_string(str2)\n",
    "        if verbose:\n",
    "            print(f\"Comparing: '{ss1}' vs '{ss2}'\")\n",
    "        return ss1 == ss2\n",
    "    except Exception:\n",
    "        return str1 == str2\n",
    "\n",
    "def compute_score(solution_str, ground_truth) -> float:\n",
    "    \"\"\"计算单个问题的得分\"\"\"\n",
    "    retval = 0.0\n",
    "    try:\n",
    "        string_in_last_boxed = last_boxed_only_string(solution_str)\n",
    "        if string_in_last_boxed is not None:\n",
    "            answer = remove_boxed(string_in_last_boxed)\n",
    "            if is_equiv(answer, ground_truth):\n",
    "                retval = 1.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_score: {e}\")\n",
    "\n",
    "    return retval, string_in_last_boxed, remove_boxed(string_in_last_boxed) if string_in_last_boxed else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75265f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:1\"\n",
    ")\n",
    "\n",
    "attetion_T = 1.05\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    model.model.layers[i].self_attn.scaling = model.model.layers[i].self_attn.scaling * 1/attetion_T\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "\n",
    "# Generation configuration\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 12000,\n",
    "    \"do_sample\": False,\n",
    "#     \"temperature\": 0.2,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "# 定义停止词\n",
    "stop_words = [\"```python\", \"```py\", \"Python code\", \"# Python\", \"import \"]\n",
    "stop_words_ids = [tokenizer.encode(word, add_special_tokens=False) for word in stop_words]\n",
    "\n",
    "# 全局变量来跟踪\\boxed的位置\n",
    "boxed_positions = {}\n",
    "\n",
    "class StoppingCriteria:\n",
    "    def __init__(self, tokenizer, max_tokens_after_boxed=100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens_after_boxed = max_tokens_after_boxed\n",
    "        self.boxed_text = \"\\\\boxed\"\n",
    "        self.boxed_tokens = tokenizer.encode(self.boxed_text, add_special_tokens=False)\n",
    "        \n",
    "        # 停止词\n",
    "        self.stop_words = [\"```python\", \"```py\", \"Python code\", \"# Python\", \"import \"]\n",
    "        self.stop_words_ids = [tokenizer.encode(word, add_special_tokens=False) for word in self.stop_words]\n",
    "    \n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        current_sequence = input_ids[0].tolist()\n",
    "        \n",
    "        # 检查停止词\n",
    "        for stop_word_ids in self.stop_words_ids:\n",
    "            if len(current_sequence) >= len(stop_word_ids):\n",
    "                if current_sequence[-len(stop_word_ids):] == stop_word_ids:\n",
    "                    return True\n",
    "        \n",
    "        # 查找所有\\boxed出现的位置\n",
    "        boxed_positions = []\n",
    "        for i in range(len(current_sequence) - len(self.boxed_tokens) + 1):\n",
    "            if current_sequence[i:i+len(self.boxed_tokens)] == self.boxed_tokens:\n",
    "                boxed_positions.append(i)\n",
    "        \n",
    "        if boxed_positions:\n",
    "            # 获取最后一个\\boxed的位置\n",
    "            last_boxed_pos = boxed_positions[0]\n",
    "            tokens_after_last_boxed = len(current_sequence) - last_boxed_pos\n",
    "            \n",
    "            # 如果\\boxed后生成的token数量超过阈值，则停止\n",
    "            if tokens_after_last_boxed > self.max_tokens_after_boxed:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# 创建停止条件实例\n",
    "stopping_criteria = StoppingCriteria(tokenizer, max_tokens_after_boxed=100)\n",
    "\n",
    "# 创建保存score序列的列表\n",
    "scores_list = []\n",
    "\n",
    "# Evaluate accuracy\n",
    "correct = 0\n",
    "total = len(dataset)\n",
    "\n",
    "for i, example in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
    "    # Construct prompt\n",
    "    prompt = (\n",
    "        \"Solve the following math problem step by step. \"\n",
    "        \"Put your final answer in a boxed format at the end.\\n\\n\"\n",
    "        f\"Question: {example['problem']}\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    try:\n",
    "        # 使用自定义停止条件\n",
    "        from transformers import StoppingCriteriaList\n",
    "        stopping_criteria_list = StoppingCriteriaList([stopping_criteria])\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            **generation_config,\n",
    "            stopping_criteria=stopping_criteria_list\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error with stopping criteria: {e}\")\n",
    "        # 回退到基本生成\n",
    "        outputs = model.generate(**inputs, **generation_config)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 只保留模型生成的部分（移除prompt）\n",
    "    model_output = response[len(prompt):]\n",
    "    \n",
    "    # 使用健壮的评分函数\n",
    "    score, boxed_content, extracted_answer = compute_score(model_output, example['answer'])\n",
    "    correct += score\n",
    "    \n",
    "    # 将当前样本的score和相关信息保存到列表中\n",
    "    scores_list.append({\n",
    "        'index': i,\n",
    "        'problem': example['problem'],\n",
    "        'model_output': model_output,\n",
    "        'boxed_content': boxed_content,\n",
    "        'extracted_answer': extracted_answer,\n",
    "        'target_answer': example['answer'],\n",
    "        'score': score\n",
    "    })\n",
    "    \n",
    "    # 调试信息\n",
    "    print(f\"\\n--- Example {i} ---\")\n",
    "    print(f\"Problem: {example['problem'][:100]}...\")\n",
    "    print(f\"Model output length: {len(model_output)} chars\")\n",
    "    \n",
    "    # 检查是否包含\\boxed\n",
    "    if \"\\\\boxed\" in model_output:\n",
    "        boxed_pos = model_output.rfind(\"\\\\boxed\")\n",
    "        chars_after_boxed = len(model_output) - boxed_pos\n",
    "        print(f\"Found \\\\boxed at position {boxed_pos}, {chars_after_boxed} chars after boxed\")\n",
    "    \n",
    "    print(f\"Extracted boxed content: {boxed_content}\")\n",
    "    print(f\"Extracted answer: {extracted_answer}\")\n",
    "    print(f\"Target answer: {example['answer']}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"\\nEvaluation completed. Accuracy: {accuracy:.2%}\")\n",
    "print(f\"\\\\boxed{{{accuracy:.4f}}}\")\n",
    "\n",
    "# 保存score序列到文件\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 生成文件名\n",
    "model_short_name = model_name.split(\"/\")[-1]  # 只取模型名称的最后部分\n",
    "dataset_name = \"MATH-500\"  # 数据集名称\n",
    "filename = f\"{model_short_name}_{dataset_name}_attention_T_{attetion_T}.json\"\n",
    "\n",
    "# 准备保存的数据\n",
    "save_data = {\n",
    "    'model_name': model_name,\n",
    "    'dataset': dataset_name,\n",
    "    'attention_T': attetion_T,\n",
    "    'total_examples': total,\n",
    "    'accuracy': accuracy,\n",
    "    'scores': scores_list\n",
    "}\n",
    "\n",
    "# 保存到文件\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nScore序列已保存到文件: {filename}\")\n",
    "print(f\"文件包含 {len(scores_list)} 个样本的详细评分信息\")\n",
    "\n",
    "# 可选：也保存一个简化的CSV文件用于分析\n",
    "import pandas as pd\n",
    "\n",
    "# 创建简化的DataFrame\n",
    "df_data = []\n",
    "for item in scores_list:\n",
    "    df_data.append({\n",
    "        'index': item['index'],\n",
    "        'score': item['score'],\n",
    "        'extracted_answer': item['extracted_answer'],\n",
    "        'target_answer': item['target_answer'],\n",
    "        'has_boxed': 1 if item['boxed_content'] else 0\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "csv_filename = f\"{model_short_name}_{dataset_name}_attention_T_{attetion_T}.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"简化版CSV文件已保存到: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb1ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 18:26:02 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 16384, 'tensor_parallel_size': 2, 'disable_log_stats': True, 'model': '/home/xsj/data_xsj/1models/Qwen3-0.6B'}\n",
      "INFO 11-23 18:26:02 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-23 18:26:02 [model.py:1745] Using max model len 16384\n",
      "INFO 11-23 18:26:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:03 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/home/xsj/data_xsj/1models/Qwen3-0.6B', speculative_config=None, tokenizer='/home/xsj/data_xsj/1models/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/xsj/data_xsj/1models/Qwen3-0.6B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m WARNING 11-23 18:26:03 [multiproc_executor.py:869] Reducing Torch parallelism from 12 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:04 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:46365 backend=nccl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:04 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:46365 backend=nccl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:05 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m WARNING 11-23 18:26:05 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.\n",
      "WARNING 11-23 18:26:05 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.6 not supported, communicator is not available.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m WARNING 11-23 18:26:05 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-23 18:26:05 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "[Gloo] Rank [Gloo] Rank 1 is connected to 10 peer ranks. Expected number of connected peer ranks is :  is connected to 11\n",
      " peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0[Gloo] Rank  is connected to 11 peer ranks.  is connected to Expected number of connected peer ranks is : 1 peer ranks. 1Expected number of connected peer ranks is : \n",
      "1\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:05 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-23 18:26:05 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank [Gloo] Rank 10 is connected to  is connected to 11 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 11\n",
      "\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:05 [gpu_model_runner.py:3259] Starting to load model /home/xsj/data_xsj/1models/Qwen3-0.6B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=594166)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:06 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "INFO 11-23 18:26:06 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=594166)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:06 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "INFO 11-23 18:26:06 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.22it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:06 [default_loader.py:314] Loading weights took 0.19 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:07 [gpu_model_runner.py:3338] Model loading took 0.5660 GiB memory and 0.545065 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:12 [backends.py:631] Using cache directory: /home/xsj/data_xsj/.cache/vllm/torch_compile_cache/9aba0b807b/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:12 [backends.py:647] Dynamo bytecode transform time: 4.76 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:14 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.261 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=594166)\u001b[0;0m INFO 11-23 18:26:14 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.284 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:15 [monitor.py:34] torch.compile takes 7.02 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:16 [gpu_worker.py:359] Available KV cache memory: 19.17 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:16 [kv_cache_utils.py:1229] GPU KV cache size: 358,944 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:16 [kv_cache_utils.py:1234] Maximum concurrency for 16,384 tokens per request: 21.91x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:16 [kv_cache_utils.py:1234] Maximum concurrency for 16,384 tokens per request: 21.91x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 29.33it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 30.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=594164)\u001b[0;0m INFO 11-23 18:26:20 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 1.09 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=594151)\u001b[0;0m INFO 11-23 18:26:20 [core.py:250] init engine (profile, create kv cache, warmup model) took 13.52 seconds\n",
      "INFO 11-23 18:26:21 [llm.py:352] Supported tasks: ['generate']\n",
      "Generating responses with vLLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 500/500 [00:00<00:00, 2975.33it/s]\n",
      "Processed prompts:  27%|██▋       | 137/500 [20:48<1:16:50, 12.70s/it, est. speed input: 9.42 toks/s, output: 538.86 toks/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 批量生成（vLLM 高效支持 batch）\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating responses with vLLM...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 处理结果\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (example, output) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(dataset, outputs)):\n",
      "File \u001b[0;32m~/data_xsj/miniconda3/envs/fast/lib/python3.10/site-packages/vllm/entrypoints/llm.py:448\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[0m\n\u001b[1;32m    438\u001b[0m lora_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_modality_specific_lora_reqs(prompts, lora_request)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    441\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mprompts,\n\u001b[1;32m    442\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority,\n\u001b[1;32m    446\u001b[0m )\n\u001b[0;32m--> 448\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/data_xsj/miniconda3/envs/fast/lib/python3.10/site-packages/vllm/entrypoints/llm.py:1738\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1736\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1738\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1739\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1740\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/data_xsj/miniconda3/envs/fast/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:285\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record_function_or_nullcontext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_engine step: get_output\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 285\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record_function_or_nullcontext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_engine step: process_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/data_xsj/miniconda3/envs/fast/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:707\u001b[0m, in \u001b[0;36mSyncMPClient.get_output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EngineCoreOutputs:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/data_xsj/miniconda3/envs/fast/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/data_xsj/miniconda3/envs/fast/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 11-23 18:47:16 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# ===== 配置 =====\n",
    "model_name = \"/home/xsj/data_xsj/1models/Qwen3-0.6B\"\n",
    "attention_T = 1.05  \n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=2 ,        # 如果只用 cuda:1，确保 CUDA_VISIBLE_DEVICES=1\n",
    "    gpu_memory_utilization=0.9,   # 防止 OOM\n",
    "    max_model_len=16384           # Qwen3 支持长上下文\n",
    ")\n",
    "\n",
    "# ===== 停止词（vLLM 支持字符串列表）=====\n",
    "stop_words = [\"```python\", \"```py\", \"Python code\", \"# Python\", \"import \"]\n",
    "\n",
    "# 注意：vLLM 的 stop 不支持 token ID，只支持字符串\n",
    "# 所以直接传字符串即可\n",
    "\n",
    "# ===== 采样参数 =====\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=12000,\n",
    "    temperature=0.8,              # do_sample=False ≈ temperature=0\n",
    "    repetition_penalty=1.1,\n",
    "    stop=stop_words,              # vLLM 原生支持\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# ===== 后处理函数（模拟你的 stopping logic）=====\n",
    "def truncate_after_boxed(text, max_tokens_after_boxed=100):\n",
    "    \"\"\"在 \\\\boxed{...} 之后最多保留 max_tokens_after_boxed 个 token\"\"\"\n",
    "    if \"\\\\boxed\" not in text:\n",
    "        return text\n",
    "    \n",
    "    # 找到最后一个 \\boxed 的位置\n",
    "    last_boxed_idx = text.rfind(\"\\\\boxed\")\n",
    "    \n",
    "    # 截取从开头到 \\boxed 之后的部分\n",
    "    prefix = text[:last_boxed_idx]\n",
    "    suffix = text[last_boxed_idx:]\n",
    "    \n",
    "    # 对 suffix 分词，限制长度\n",
    "    suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "    if len(suffix_tokens) > max_tokens_after_boxed:\n",
    "        suffix_truncated = tokenizer.decode(suffix_tokens[:max_tokens_after_boxed])\n",
    "        return prefix + suffix_truncated\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# ===== 推理循环 =====\n",
    "scores_list = []\n",
    "correct = 0\n",
    "total = len(dataset)\n",
    "\n",
    "prompts = []\n",
    "indices = []\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    prompt = (\n",
    "        \"Solve the following math problem step by step. \"\n",
    "        \"Put your final answer in a boxed format at the end.\\n\\n\"\n",
    "        f\"Question: {example['problem']}\\n\\n\"\n",
    "    )\n",
    "    prompts.append(prompt)\n",
    "    indices.append(i)\n",
    "\n",
    "# 批量生成（vLLM 高效支持 batch）\n",
    "print(\"Generating responses with vLLM...\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# 处理结果\n",
    "for i, (example, output) in enumerate(zip(dataset, outputs)):\n",
    "    raw_output = output.outputs[0].text  # vLLM 返回的是 CompletionOutput\n",
    "    \n",
    "    # 应用 \\boxed 后截断逻辑\n",
    "    truncated_output = truncate_after_boxed(raw_output, max_tokens_after_boxed=100)\n",
    "    \n",
    "    # 评分\n",
    "    score, boxed_content, extracted_answer = compute_score(truncated_output, example['answer'])\n",
    "    correct += score\n",
    "    \n",
    "    scores_list.append({\n",
    "        'index': indices[i],\n",
    "        'problem': example['problem'],\n",
    "        'model_output': truncated_output,\n",
    "        'raw_output': raw_output,  # 可选：保存原始输出\n",
    "        'boxed_content': boxed_content,\n",
    "        'extracted_answer': extracted_answer,\n",
    "        'target_answer': example['answer'],\n",
    "        'score': score\n",
    "    })\n",
    "\n",
    "    # 调试打印（可选）\n",
    "    if i < 3:  # 只打印前几个\n",
    "        print(f\"\\n--- Example {i} ---\")\n",
    "        print(f\"Problem: {example['problem'][:100]}...\")\n",
    "        print(f\"Model output: {truncated_output[:200]}...\")\n",
    "        print(f\"Score: {score}\")\n",
    "\n",
    "# ===== 计算准确率 & 保存结果 =====\n",
    "accuracy = correct / total\n",
    "print(f\"\\nEvaluation completed. Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# 保存文件\n",
    "model_short_name = model_name.split(\"/\")[-1]\n",
    "filename_base = f\"{model_short_name}_MATH-500_attention_T_{attention_T}\"\n",
    "\n",
    "# JSON\n",
    "save_data = {\n",
    "    'model_name': model_name,\n",
    "    'dataset': \"MATH-500\",\n",
    "    'attention_T': attention_T,\n",
    "    'total_examples': total,\n",
    "    'accuracy': accuracy,\n",
    "    'scores': scores_list\n",
    "}\n",
    "with open(filename_base + \".json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# CSV\n",
    "df = pd.DataFrame([{\n",
    "    'index': item['index'],\n",
    "    'score': item['score'],\n",
    "    'extracted_answer': item['extracted_answer'],\n",
    "    'target_answer': item['target_answer'],\n",
    "    'has_boxed': 1 if item['boxed_content'] else 0\n",
    "} for item in scores_list])\n",
    "df.to_csv(filename_base + \".csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Results saved to {filename_base}.json and .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51c5e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xsj/data_xsj/miniconda3/envs/fast/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 8\n",
      "Max: 791\n",
      "Mean: 70.9\n",
      "Median: 50.0\n",
      "90th percentile: 141.1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"/home/xsj/data_xsj/1models/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "\n",
    "lengths = []\n",
    "for example in dataset:\n",
    "    tokens = tokenizer.encode(example[\"problem\"], add_special_tokens=False)\n",
    "    lengths.append(len(tokens))\n",
    "\n",
    "import numpy as np\n",
    "print(f\"Min: {np.min(lengths)}\")\n",
    "print(f\"Max: {np.max(lengths)}\")\n",
    "print(f\"Mean: {np.mean(lengths):.1f}\")\n",
    "print(f\"Median: {np.median(lengths):.1f}\")\n",
    "print(f\"90th percentile: {np.percentile(lengths, 90):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f298987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
